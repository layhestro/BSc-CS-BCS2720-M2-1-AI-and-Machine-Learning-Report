\begin{abstract}
Reinforcement Learning (RL) training is computationally expensive and unpredictable, hindering effective resource allocation and hyperparameter optimization. We present a meta-learning approach that uses supervised machine learning to predict properties of deep reinforcement learning training runs before execution. Specifically, we address three critical research questions: predicting (1) wall-clock training duration, (2) final agent performance, and (3) peak RAM usage. Using Unity ML-Agents, we collected training data across multiple environments (e.g., 3DBall, GridWorld) and algorithms (PPO, SAC). Our experiments with varying regression models demonstrate that meta-learning is a viable strategy for resource estimation. For training duration, Gradient Boosting achieved the best practical accuracy with a Mean Absolute Error (MAE) of approximately 2-3 minutes. For final performance (reward), tree-based ensembles like Random Forest and Extra Trees proved most effective at capturing non-linear hyperparameter interactions. Finally, for peak RAM usage, we achieved high predictive accuracy ($R^2 \approx 0.99$ for PPO) using K-Nearest Neighbors and Random Forest models. These results highlight the potential of data-driven prediction to accelerate RL research cycles and improve resource scheduling.
\end{abstract}

\begin{IEEEkeywords}
meta-learning, reinforcement learning, machine learning, Unity ML-Agents, training prediction, hyperparameter optimization
\end{IEEEkeywords}
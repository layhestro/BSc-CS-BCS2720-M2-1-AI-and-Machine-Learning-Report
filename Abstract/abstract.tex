\begin{abstract}
Reinforcement Learning (RL) training is computationally expensive, with runs ranging from minutes to days depending on environment complexity and hyperparameter configurations. This unpredictability hinders effective resource allocation and hyperparameter optimization in large-scale RL research. We present a meta-learning approach that uses supervised machine learning to predict properties of deep reinforcement learning training runs before execution. Specifically, we address two critical research questions: (1) predicting wall-clock training duration based on configuration parameters and hardware specifications, and (2) predicting final agent performance from training configurations. Using Unity ML-Agents as our experimental platform, we collected training data across multiple environments (3DBall, GridWorld, Hallway, PushBlock, Pyramids, Walker) with varying hyperparameters and algorithms (PPO, SAC, POCA). Our experiments with multiple regression models show that Gradient Boosting Regressor achieves the best performance for duration prediction with MAE of 0.13 (log-seconds) and R² of 0.9501, significantly outperforming Ridge Regression (R² = 0.8383). Feature importance analysis reveals that \texttt{max\_steps}, environment complexity, and neural network architecture (\texttt{hidden\_units}, \texttt{num\_layers}) are the primary predictors of training duration. These results demonstrate the feasibility of meta-learning for RL training prediction, enabling more efficient resource scheduling and accelerated hyperparameter search in reinforcement learning research.
\end{abstract}

\begin{IEEEkeywords}
meta-learning, reinforcement learning, machine learning, Unity ML-Agents, training prediction, hyperparameter optimization
\end{IEEEkeywords}
\section{Experiments}

\subsection{Dataset Statistics}

Our dataset comprises training runs collected across multiple Unity ML-Agents environments with systematically varied hyperparameters. Table \ref{tab:dataset_stats} summarizes the dataset characteristics.

\begin{table}[h]
\centering
\caption{Dataset Statistics}
\label{tab:dataset_stats}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Characteristic} & \textbf{Value} \\
\midrule
Total Training Runs & 5,799 \\
Features (Input) & 50 columns \\
Environments & 7 (3DBall, GridWorld, Hallway, etc.) \\
Algorithms & 3 (PPO, SAC, POCA) \\
Duration Range & Minutes to hours \\
Hardware & Consistent (RTX 4070 Ti, 64GB RAM) \\
\bottomrule
\end{tabular}
\end{table}

The large dataset size (5,799 runs) provides substantial statistical power for training and evaluating our predictive models. Consistent hardware specifications across runs allow the models to focus on learning the effects of hyperparameters and environment characteristics rather than hardware variability.

\subsection{Experimental Setup}

\subsubsection{Model Configurations}

We evaluated five regression models for Research Question 1 (Training Duration Prediction):

\begin{enumerate}
    \item \textbf{Random Forest Regressor}: 100 trees, all CPU cores utilized, default scikit-learn parameters
    
    \item \textbf{Ridge Regression}: L2 regularization with tuned $\alpha$ parameter, combined with StandardScaler and OneHotEncoder in a scikit-learn Pipeline
    
    \item \textbf{Gradient Boosting Regressor}: Sequential ensemble method, default scikit-learn parameters
    
    \item \textbf{HistGradient Boosting Regressor}: Modern histogram-based gradient boosting, handles categorical features natively
    
    \item \textbf{K-Nearest Neighbors}: Distance-weighted regression with tuned $k$, StandardScaler preprocessing
\end{enumerate}

All models were trained using log-transformed target values: $y = \log(1 + \text{training\_duration\_seconds})$.

\subsubsection{Evaluation Protocol}

\begin{itemize}
    \item \textbf{Cross-Validation}: 5-fold CV with shuffling (seed=42)
    \item \textbf{Metrics}: Mean Absolute Error (MAE) in log-seconds, R² score
    \item \textbf{Reporting}: Mean and standard deviation across folds
    \item \textbf{Statistical Significance}: Models compared based on average R² score
\end{itemize}

\subsection{Results: Research Question 1}

\subsubsection{Model Performance Comparison}

Table \ref{tab:rq1_results} presents the performance of all evaluated models for training duration prediction.

\begin{table}[h]
\centering
\caption{Training Duration Prediction Results}
\label{tab:rq1_results}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model} & \textbf{MAE (log-sec)} & \textbf{R² Score} \\
\midrule
Random Forest & 0.13 & 0.9381 \\
Ridge Regression & 0.43 & 0.8383 \\
Gradient Boosting & \textbf{0.13} & \textbf{0.9501} \\
HistGradient Boosting & 847.34 & 0.1339 \\
K-Nearest Neighbors & TBD & TBD \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}

\begin{itemize}
    \item \textbf{Gradient Boosting achieves best performance}: With MAE of 0.13 log-seconds and R² of 0.9501, it explains 95\% of variance in training duration, significantly outperforming other models.
    
    \item \textbf{Random Forest performs comparably}: Very close performance (R² = 0.9381, MAE = 0.13) with faster training time, making it a viable alternative.
    
    \item \textbf{Ridge Regression establishes baseline}: Linear model achieves respectable R² of 0.8383, indicating that linear relationships capture much of the signal, though non-linear models improve substantially.
    
    \item \textbf{HistGradient Boosting underperforms}: Poor performance (R² = 0.1339) suggests that the categorical encoding approach or default parameters are not well-suited for this dataset. This unexpected result warrants further investigation.
\end{itemize}

\subsubsection{Interpretation of Results}

The MAE of 0.13 log-seconds for Gradient Boosting translates to approximately $e^{0.13} \approx 1.14\times$ multiplicative error in the original time scale. For a 1-hour (3600 seconds) training run, this corresponds to roughly $\pm 8$ minutes prediction error, which is highly practical for resource scheduling decisions.

The high R² score (0.9501) indicates that configuration parameters and hardware specifications are highly predictive of training duration, validating our meta-learning approach. Only 5\% of variance remains unexplained, likely due to stochastic factors in RL training (random seed effects, environment initialization) and system-level variations (background processes, thermal throttling).

\subsubsection{Feature Importance Analysis}

Figure \ref{fig:feature_importance} (to be generated) shows the top 10 most important features for duration prediction in the Gradient Boosting model.

\textbf{Primary Predictors:}
\begin{itemize}
    \item \texttt{max\_steps}: Strongest predictor, as it directly determines the total number of training iterations
    \item \texttt{environment}: Different environments have vastly different computational costs per step
    \item \texttt{hidden\_units} and \texttt{num\_layers}: Network size directly impacts forward/backward pass time
\end{itemize}

\textbf{Secondary Predictors:}
\begin{itemize}
    \item \texttt{batch\_size}: Affects GPU utilization and memory bandwidth
    \item \texttt{num\_epoch}: Direct multiplier on training time (e.g., 10 epochs $\approx$ 10× duration)
    \item \texttt{buffer\_size}: Larger buffers require more memory operations
\end{itemize}

\textbf{Minimal Impact:}
\begin{itemize}
    \item \texttt{learning\_rate}: Does not affect computational cost directly
    \item \texttt{gamma}, \texttt{epsilon}: Hyperparameters that influence learning dynamics but not speed
\end{itemize}

\subsection{Preliminary Results: Research Question 2}

Work on predicting final agent performance (Research Question 2) is currently in progress. Initial implementation using K-Nearest Neighbors has been completed, but comprehensive evaluation across multiple models remains to be conducted. Early challenges include:

\begin{itemize}
    \item Performance varies dramatically across environments (e.g., 3DBall converges to +1.0, while complex environments show different reward scales)
    \item Reward normalization strategies need investigation
    \item More training data may be required for robust performance prediction
\end{itemize}

\subsubsection{Key findings}
\begin{itemize}
    \item \textbf{Linear regression}: In terms of both algorithms Linear regression is not optimal, however its vital to check , as it proves that the data does not interact linearly.
    \item \textbf{Artificial Neural Networks(MLP Regressor)}: could be a good model if not for the resource constraints. In terms of results , it performed below average compared to other models , for both ppo and sac.
    \item \textbf{RandomForestRegressor and ExtraTrees} are the best models in terms of results for both algorithms. Their robust nature and randomness are good features for nonlinear data and its interactions.
    \item \textbf{HistGradientRegressor} is the only model that came close to random forest and extra tree , as the more data it has , the better it scales.
\end{itemize}

\subsection{Preliminary Results: Research Question 3}

\subsubsection{Model Performance}
Figures [\ref{fig:PPOResultsRQ3}][\ref{fig:SACResultsRQ3}] present the performance of all training models evaluated that predict the maximum RAM consumption. The LR will be used as a a base-line.

\subsubsection{Key Findings}

\begin{itemize}
    \item \textbf{K-NN, RFR and ETR are the best models for prediction of RAM in our case}: These models have comparable and maybe even interchangeable results, i.e very similar R squared and MAE.
    \item \textbf{K-nn is very sensitive to the data leakage}: during our experiments we have observed that even a slight data leakage gave K-NN an edge over RFR and ETR.
    \item \textbf{High R squared score for a LR model}: this suggests that features and RAM usage follow a strong but not perfect linear trend, due to a comparably high MAE. Which suggests high noise
    \item \textbf{Possible Over-Evaluation with R squared}: we can observe very high, if not unrealistic R squared score. It means that the metric is not ideal thus overlying on it is not optimal, instead better idea is relying more on MAE and compare the performance of different models.
    \item \textbf{ANN performed the worst}: this could be due to the fact there isn't sufficient amount of data or required hyper-parameter tuning for ANN.
    
\end{itemize}

\subsubsection{Interpretation of Results}
The results suggest multiple things:

\begin{enumerate}
    \item The relationship is very linear
    \item Data last a lot of related data due to K-NN performing sufficiently, especiall with PPO.
\end{enumerate}



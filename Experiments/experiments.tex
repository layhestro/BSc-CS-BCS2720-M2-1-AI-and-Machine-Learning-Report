\section{Experiments}

\subsection{Experimental Setup \& Reproducibility}
To ensure all experiments can be replicated \cite{pedregosa2011scikit}, we adhered to the following protocols across all research questions:
\begin{itemize}
    \item \textbf{Random Seed}: Fixed random seed (42) for all stochastic operations (data shuffling, cross-validation splits).
    \item \textbf{Validation Splits}: Identical cross-validation splits used across all models for fair comparison.
    \item \textbf{Software}: Documented versions (Python 3, pandas 2.2.2, numpy 2.0.2, scikit-learn).
    \item \textbf{Code & Storage}: Version-controlled code in Jupyter notebooks; trained models saved with joblib.
    \item \textbf{Compute}: All experiments executed on Google Colab with consistent resources.
\end{itemize}

\subsection{Research Question 1: Duration Prediction}

\subsubsection{Dataset Characteristics}
After data cleaning, the duration prediction dataset contains 5,673 valid training runs with 24 predictors (2 categorical, 22 numeric). Durations range from 364 seconds (6 minutes) to 70,560 seconds (19.6 hours). This 200x range motivated the log transformation approach.

\subsubsection{Experiment 1: Model Family Comparison}
\textbf{Objective}: Determine which model family performs best.
\textbf{Method}: Train representative models from Linear, Tree-based, and Distance-based families.
\textbf{Evaluation}: 5-fold stratified cross-validation with MAE and R² score.

\subsubsection{Experiment 2: Hyperparameter Sensitivity}
\textbf{Objective}: Assess gains from tuning.
\textbf{Method}: Compare default vs. tuned configurations for Ridge, Random Forest, KNN, and HistGradient Boosting using GridSearchCV.

\subsubsection{Experiment 3: Feature Importance Analysis}
\textbf{Objective}: Identify influential parameters.
\textbf{Method}: Extract importance scores from the best tree-based model (Gradient Boosting).

\subsubsection{Experiment 4: Prediction Error Analysis}
\textbf{Objective}: Characterize model errors.
\textbf{Method}: Generate residual plots and error distributions for the champion model.

\subsection{Research Question 2: Performance Prediction}

\subsubsection{Dataset Characteristics}
The dataset target is \texttt{final\_mean\_reward}. We use separate datasets for PPO and SAC due to severe data imbalance (see Figure~\ref{fig:algfreq}). Post-training variables were excluded to prevent leakage. Reward values vary significantly across environments (e.g., 3DBall vs. multi-agent tasks).

\subsubsection{Experiment 1: Model Family Comparison}
\textbf{Objective}: Identify the best regression architecture.
\textbf{Method}: Train Distance-based (KNN), Linear, Neural Network (MLP), and Tree Ensembles (Random Forest, Extra Trees, HistGradientBoosting) on algorithm-specific data.
\textbf{Evaluation}: 10-fold cross-validation with R² score and MAE.

\subsubsection{Experiment 2: Algorithm Separation Validation}
\textbf{Objective}: Validate the decision to train separate models.
\textbf{Method}: Train algorithm-specific models (PPO vs. SAC) and evaluate on corresponding test data.
\textbf{Rationale}: PPO and SAC have fundamentally different learning dynamics; shared hyperparameters affect them differently.

\subsubsection{Experiment 3: Cross-Validation Stability Analysis}
\textbf{Objective}: Assess model consistency.
\textbf{Method}: Calculate mean and standard deviation of R² and MAE across 10 folds.

\subsubsection{Experimental Challenges}
Two main challenges affect this research question:
\textbf{Algorithm Imbalance}: PPO runs vastly outnumber SAC runs, potentially making SAC models less reliable.
\textbf{Environment Diversity}: Overrepresentation of simple environments (3DBall) could bias models, reducing accuracy on underrepresented, complex tasks.

\subsection{Research Question 3: RAM Prediction}

\subsubsection{Dataset Characteristics}
Data processing followed the same protocol as RQ2, targeting \texttt{peak\_ram\_mb}. Features were selected to avoid data leakage.

\subsubsection{Experiment 1: Model Evaluation}
\textbf{Objective}: Identify the best model for RAM prediction.
\textbf{Method}: Train models mentioned in Methodology and Implementation (\ref{methodsrq3}, \ref{implmodelselectionrq3}).
\textbf{Evaluation}: 10-fold cross-validation with R² and MAE.

\subsubsection{Experiment 2: Algorithm Separation Validation}
\textbf{Objective}: Validate separate models for PPO and SAC.
\textbf{Rationale}: The data imbalance and differing algorithm dynamics necessitate separation. The lack of SAC configuration hyperparameters in the dataset may lead to inconsistent or biased predictions.
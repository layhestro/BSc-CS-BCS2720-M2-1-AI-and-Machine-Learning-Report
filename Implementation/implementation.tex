\section{Implementation}

This section describes our experimental platform, data collection pipeline, preprocessing steps, and model training infrastructure.

\subsection{Experimental Platform}

We use Unity ML-Agents \cite{juliani2018unity}, an open-source toolkit for training intelligent agents in Unity environments. ML-Agents provides:

\begin{itemize}
    \item Multiple pre-built RL environments with varying complexity levels
    \item Implementations of state-of-the-art RL algorithms (PPO, SAC, POCA)
    \item Comprehensive logging of training metrics and hyperparameters
    \item Support for both local and cloud-based training
\end{itemize}

Our implementation is based on a fork of the ML-Agents repository (version 21, fix-numpy-release-21-branch), customized to extract additional metadata for our meta-learning purposes.

\subsection{Training Environments}

We collected data across seven diverse Unity environments, each presenting different challenges:

\begin{itemize}
    \item \textbf{3DBall}: Agent balances a ball on a platform by rotating. Simple continuous control with fast convergence (approx. 120,000 steps).
    
    \item \textbf{GridWorld}: Discrete navigation environment where agent collects rewards in a grid.
    
    \item \textbf{Hallway}: Agent navigates corridors to reach goal positions, testing spatial reasoning.
    
    \item \textbf{PushBlock}: Agent must push blocks to target locations, requiring physical reasoning.
    
    \item \textbf{Pyramids}: Multi-agent environment with button-pressing tasks.
    
    \item \textbf{Walker}: Bipedal locomotion task with high-dimensional action and observation spaces.
    
    \item \textbf{Basic}: Simple demonstration environment for testing configurations.
\end{itemize}

These environments span a range of observation space dimensions, action space sizes, and task complexities, ensuring our models generalize across different RL problem types.

\subsection{Data Collection Pipeline}

Our data collection pipeline consists of three main components:

\subsubsection{Training Execution}

Training runs were executed in two modes:

\begin{enumerate}
    \item \textbf{Local Training}: Initial experiments on high-performance hardware (NVIDIA RTX 4070 Ti, 64GB RAM, 28 CPU cores) running Windows.
    
    \item \textbf{Cloud Training}: Later experiments on Google Colab Pro using Linux dedicated server builds with headless rendering (\texttt{--no-graphics} flag) for efficient resource utilization.
\end{enumerate}

Each training run was configured via YAML files specifying all hyperparameters for the selected algorithm (PPO, SAC, or POCA).

\subsubsection{Metadata Extraction}

We developed a custom data collection package that extracts:

\begin{itemize}
    \item \textbf{Configuration Parameters}: All hyperparameters from YAML config files (learning rate, batch size, network architecture, algorithm-specific parameters)
    
    \item \textbf{Hardware Specifications}: CPU cores, RAM, GPU availability and memory, operating system
    
    \item \textbf{Training Outcomes}: Duration in seconds (wall-clock time), final performance metrics (mean reward, entropy, value estimates), resource usage (peak RAM)
    
    \item \textbf{Temporal Data}: Training start timestamp, total steps executed
\end{itemize}

The extraction process leverages ML-Agents' existing logging infrastructure (\texttt{timers.json}, \texttt{training\_status.json}, TensorBoard event files) and augments it with system-level metrics collected during training.

\subsubsection{Data Storage}

Each training run generates one row in our dataset, combining all extracted features and outcomes. Data is stored in CSV format with 50 columns capturing comprehensive information about each training run. The schema includes:

\begin{itemize}
    \item Categorical features: \texttt{environment}, \texttt{algorithm}, \texttt{os}, \texttt{gpu\_name}
    \item Numeric features: All hyperparameters, hardware specs, training outcomes
    \item Boolean features: \texttt{gpu\_available}
    \item Temporal features: \texttt{timestamp}
\end{itemize}

\subsection{Data Preprocessing}

Data preprocessing follows a systematic pipeline implemented in Google Colab:

\subsubsection{Data Loading and Merging}

Multiple CSV files from different training sessions are loaded and concatenated into a single master dataset. We handle the typo in the original column name (\texttt{enviroment} â†’ \texttt{environment}) to ensure consistency.

\subsubsection{Feature Selection}

Based on our research questions, we select relevant features:

\begin{itemize}
    \item \textbf{For Duration Prediction (RQ1)}: 22 input features including environment, algorithm, core hyperparameters, and hardware specifications
    
    \item \textbf{For Performance Prediction (RQ2)}: Extended feature set including additional training parameters (time\_horizon, buffer\_size, entropy coefficients)
\end{itemize}

Target columns are separated, and metadata fields (timestamps) are excluded from the feature set.

\subsubsection{Missing Value Handling}

Missing values are handled using domain-appropriate strategies:

\begin{itemize}
    \item Hardware features: \texttt{gpu\_available} defaults to False, \texttt{gpu\_memory\_gb} defaults to 0
    \item Numeric features: Filled with 0 (reasonable for optional hyperparameters)
    \item Target variable: Rows with missing \texttt{training\_duration\_seconds} are dropped
\end{itemize}

\subsubsection{Categorical Encoding}

Categorical variables (\texttt{environment}, \texttt{algorithm}, \texttt{os}) are encoded using two approaches depending on the model:

\begin{itemize}
    \item \textbf{Label Encoding}: For tree-based models (Random Forest, Gradient Boosting) that can handle ordinal relationships
    
    \item \textbf{One-Hot Encoding}: For linear models (Ridge) and distance-based models (KNN) to avoid imposing artificial ordering
\end{itemize}

\subsubsection{Target Transformation}

For duration prediction, we apply log transformation to the target variable:

$$y_{transformed} = \log(1 + y_{original})$$

This transformation addresses the wide range of training durations (from minutes to hours) and stabilizes variance, improving model performance. The $\log(1 + x)$ form handles zero values gracefully.

\subsection{Model Training Pipeline}

Our model training follows a rigorous evaluation protocol:

\subsubsection{Cross-Validation Strategy}

We employ 5-fold cross-validation with shuffling (random seed = 42) for all models. This ensures:

\begin{itemize}
    \item Robust performance estimates across different data splits
    \item Detection of overfitting or high variance
    \item Fair comparison between different model architectures
\end{itemize}

Each fold trains on 80\% of data and evaluates on the remaining 20\%, with performance metrics aggregated across all folds.

\subsubsection{Hyperparameter Tuning}

For models requiring hyperparameter optimization:

\begin{itemize}
    \item \textbf{Ridge Regression}: GridSearchCV over regularization parameter $\alpha \in \{0.01, 0.1, 1, 3.16, 10, 31.6, 100\}$ with 3-fold inner cross-validation
    
    \item \textbf{KNN}: Grid search over neighbors $k \in \{3, 5, 7, 9, 15, 25, 35, 50\}$, weights $\in \{uniform, distance\}$, and distance metric $p \in \{1, 2\}$
\end{itemize}

Tree-based models (Random Forest, Gradient Boosting) use default scikit-learn hyperparameters initially, as they typically perform well out-of-the-box for tabular data.

\subsubsection{Model Persistence}

After cross-validation, each model is retrained on the complete dataset to maximize available training data for deployment. Models are serialized using \texttt{joblib} and saved with associated encoders and preprocessing metadata for reproducible predictions.

\subsubsection{Feature Importance Analysis}

For tree-based models, we extract feature importance scores based on the reduction in node impurity (Gini importance). For linear models, we examine coefficient magnitudes to identify influential features. These analyses guide understanding of which configuration parameters most strongly impact training outcomes.

\subsection{Computational Infrastructure}

\textbf{Development Environment:} Google Colab Pro with GPU acceleration, Python 3.10, scikit-learn 1.x, pandas, numpy

\textbf{File Management:} Google Drive integration for persistent storage of datasets, trained models, and visualizations

\textbf{Reproducibility:} Fixed random seeds (42) throughout preprocessing and model training, versioned code in GitHub repository

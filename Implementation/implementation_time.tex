\subsection{Research Question 1: Duration Prediction Implementation}

This section describes the software implementation for predicting RL training duration based on configuration parameters and hardware specifications.

\subsubsection{Software Environment}

The duration prediction pipeline was implemented in Python using Google Colab with the following core dependencies:

\begin{itemize}
    \item \textbf{Data Processing}: pandas 2.2.2, numpy 2.0.2
    \item \textbf{Machine Learning}: scikit-learn (recent version) for all models and preprocessing
    \item \textbf{Visualization}: matplotlib, seaborn
    \item \textbf{Storage}: Google Drive integration for persistent data and model storage
    \item \textbf{Utilities}: joblib for model serialization
\end{itemize}

All experiments used a fixed random seed (42) for reproducibility across runs.

\subsubsection{Data Pipeline Architecture}

The implementation follows a modular pipeline structure with distinct stages:

\paragraph{Stage 1: Data Loading and Cleaning}
The master dataset containing 5,799 training runs is loaded from CSV format. Missing values are handled through domain-specific imputation:
\begin{itemize}
    \item Hardware features (\texttt{gpu\_available}, \texttt{gpu\_memory\_gb}) default to False and 0
    \item Numeric hyperparameters filled with 0 (appropriate for optional parameters)
    \item Rows with missing target values (\texttt{training\_duration\_seconds}) are dropped
\end{itemize}

After cleaning, the dataset contains 5,673 valid training runs with complete feature and target information.

\paragraph{Stage 2: Feature Engineering}
To prevent data leakage, features that could only be known after training completion are excluded from the predictor set. Specifically excluded:
\begin{itemize}
    \item Performance metrics: \texttt{final\_mean\_reward}, \texttt{final\_std\_reward}
    \item Resource usage: \texttt{peak\_ram\_mb}
    \item Training metadata: \texttt{total\_steps\_actual}, \texttt{timestamp}
\end{itemize}

The final feature set contains 24 predictors including categorical features (\texttt{environment}, \texttt{algorithm}, \texttt{os}, \texttt{gpu\_name}) and numeric features (hyperparameters and hardware specifications).

\paragraph{Stage 3: Target Transformation}
Training durations range from 364 seconds (6 minutes) to 70,560 seconds (19.6 hours), spanning approximately 200-fold variation. To stabilize variance and improve model performance, we apply log transformation:

\begin{equation}
y_{transformed} = \log(1 + y_{seconds})
\end{equation}

The $\log(1 + x)$ formulation handles zero values gracefully while compressing the range of duration values.

\paragraph{Stage 4: Preprocessing Pipeline}
Categorical variables are encoded using OrdinalEncoder, converting environment names and algorithm types to integer representations. This encoding is appropriate for tree-based models that can learn non-linear relationships between categories. No standardization is applied to numeric features, as tree-based models are scale-invariant.

\subsubsection{Cross-Validation Strategy}

To ensure robust performance estimates, we implement stratified 5-fold cross-validation. The stratification strategy divides training durations into quintiles, ensuring each fold contains a balanced mix of short, medium, and long training runs. This prevents individual folds from being dominated by a single duration range.

The cross-validation process:
\begin{enumerate}
    \item Split data into 5 folds (80\% train, 20\% validation per fold)
    \item For each fold:
    \begin{itemize}
        \item Train model on 4 folds
        \item Evaluate on held-out fold
        \item Record MAE and RÂ² score
    \end{itemize}
    \item Aggregate metrics: compute mean and standard deviation across folds
\end{enumerate}

\subsubsection{Model Registry}

We implement a model registry pattern to systematically compare algorithms:

\textbf{Linear Models:}
\begin{itemize}
    \item \textbf{Linear Regression}: Baseline model with no regularization
    \item \textbf{Ridge Regression}: L2 regularization with GridSearchCV over $\alpha \in \{0.01, 0.1, 1, 3.16, 10, 31.6, 100\}$
\end{itemize}

\textbf{Tree-Based Ensembles:}
\begin{itemize}
    \item \textbf{Random Forest}: 100 trees with tuned hyperparameters (max depth, min samples leaf)
    \item \textbf{Extra Trees}: 100 extremely randomized trees
    \item \textbf{Gradient Boosting}: Sequential boosting with default scikit-learn parameters
    \item \textbf{HistGradient Boosting}: Histogram-based gradient boosting with native categorical support, tested with both default and tuned parameters
\end{itemize}

\textbf{Distance-Based Models:}
\begin{itemize}
    \item \textbf{K-Nearest Neighbors}: Grid search over $k \in \{3, 5, 7, 9, 15, 25, 35, 50\}$ with distance weighting
\end{itemize}

Each model is trained using the same preprocessing pipeline and cross-validation protocol to ensure fair comparison.

\subsubsection{Model Persistence and Outputs}

After cross-validation, the best-performing model (Gradient Boosting) is retrained on the complete dataset to maximize available training data. The trained model is serialized using joblib and saved with preprocessing metadata for deployment.

The implementation generates three categories of outputs:

\begin{enumerate}
    \item \textbf{Results Files}: CSV tables with model comparison metrics, fold-by-fold performance, and feature importance scores
    \item \textbf{Visualizations}: Performance comparison plots, actual vs predicted scatter plots, residual distributions, and feature importance charts
    \item \textbf{Trained Models}: Serialized model objects with preprocessing pipelines for each tested algorithm
\end{enumerate}

All outputs are stored in organized directories (Results, Visuals, Models) within the Google Drive project folder for easy access and version control.
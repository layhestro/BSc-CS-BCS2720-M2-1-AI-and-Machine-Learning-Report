\section{Method}

This section formalizes our research questions and describes the machine learning approaches used to address them.

\subsection{Research Question 1: Training Duration Prediction}

\textbf{Problem Statement:} Can we predict wall-clock training duration based on configuration parameters and hardware specifications?
\textbf{Motivation:} RL training is computationally expensive. Accurate duration estimates would enable better resource scheduling and budget allocation, saving time and energy.

\textbf{Formalization:} We formulate this as a regression problem where:

\textbf{Input Features ($X$):}
\begin{itemize}
    \item \texttt{environment}: Categorical (15 Unity environments)
    \item \texttt{algorithm}: Categorical (PPO \cite{schulman2017proximal}, SAC \cite{haarnoja2018soft})
    \item \texttt{learning\_rate}: Float
    \item \texttt{batch\_size}: Integer
    \item \texttt{hidden\_units}: Integer
    \item \texttt{num\_layers}: Integer
    \item \texttt{max\_steps}: Integer
    \item \texttt{ram\_gb}: Float (total system RAM)
    \item \texttt{cpu\_cores}: Integer (number of available CPU cores)
\end{itemize}

\textbf{Target Variable ($y$):}
\begin{itemize}
    \item \texttt{training\_duration\_seconds}: Float (wall-clock time)
\end{itemize}

\textbf{ML Approach:} We evaluate Linear Regression, Ridge Regression, Random Forest, Gradient Boosting, and KNN using 5-fold cross-validation with MAE and R² score. We apply log transformation to the target variable: $y_{transformed} = \log(1 + y)$.

\subsection{Research Question 2: Final Performance Prediction}

\textbf{Problem Statement:} Can we predict final agent performance (mean cumulative reward) based on training configuration?
\textbf{Motivation:} Estimating performance without full training speeds up hyperparameter optimization.

\textbf{Formalization:} This is a regression problem where:

\textbf{Input Features ($X$):}
\begin{itemize}
    \item Environment and algorithm features (same as RQ1)
    \item Core hyperparameters: \texttt{learning\_rate}, \texttt{batch\_size}, \texttt{hidden\_units}, \texttt{num\_layers}, \texttt{max\_steps}
    \item Training parameters: \texttt{time\_horizon}, \texttt{buffer\_size}
    \item PPO-specific: \texttt{beta}, \texttt{epsilon}, \texttt{lambd}, \texttt{num\_epoch}
    \item SAC-specific: \texttt{tau}, \texttt{init\_entcoef}
    \item Reward shaping: \texttt{gamma}, \texttt{strength}
\end{itemize}

\textbf{Target Variable ($y$):}
\begin{itemize}
    \item \texttt{final\_mean\_reward}: Float (average cumulative reward over last 100 episodes)
\end{itemize}

\textbf{ML Approach:} We test Random Forest, Gradient Boosting, and Neural Networks (MLPRegressor) with regularization using cross-validation.

\subsection{Research Question 3: RAM Usage Prediction}\label{methodsrq3}

\textbf{Problem Statement:} Can we predict peak RAM usage during training based on environment configuration and hyperparameters?
\textbf{Motivation:} Predictive RAM models help prevent out-of-memory errors and guide resource allocation.

\textbf{Formalization:} This is a regression problem where:

\textbf{Input Features ($X$):}
\begin{itemize}
    \item \texttt{environment}: Categorical
    \item \texttt{algorithm}: Categorical (PPO, SAC)
    \item \texttt{batch\_size}: Integer
    \item \texttt{buffer\_size}: Integer
    \item \texttt{hidden\_units}: Integer
    \item \texttt{num\_layers}: Integer
    \item \texttt{ram\_gb}: Float (total available system RAM)
    \item \texttt{cpu\_cores}: Integer (number of available CPU cores)
\end{itemize}

\textbf{Target Variable ($y$):}
\begin{itemize}
    \item \texttt{peak\_ram\_mb}: Float (maximum RAM usage in MB)
\end{itemize}

\textbf{ML Approach:} We compare Linear Regression, Random Forest\cite{breiman2001random}, Extra Trees, KNN, HistGradient Boosting, and MLPRegressor.

\subsection{Evaluation Metrics}
We evaluate model performance using:
\begin{itemize}
    \item \textbf{Mean Absolute Error (MAE)}: Measures average prediction error magnitude.
    \item \textbf{R² Score}: Measures the proportion of variance explained.
\end{itemize}
If the R² score is suspiciously high, we rely on MAE entirely.